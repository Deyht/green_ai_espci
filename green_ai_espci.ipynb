{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855cdd86",
   "metadata": {
    "id": "855cdd86"
   },
   "source": [
    "# Toward AI Sustainability: Low-Level Optimization for High Impact\n",
    "\n",
    "**ESPCI 2025: Practical work guide**\n",
    "\n",
    "This document presents instructions and questions regarding the practical work sessions of this course. All the materials (slides, notebooks, base codes) can be recovered from the corresponding [GitHub repository](https://github.com/Deyht/green_ai).\n",
    "\n",
    "The notebook can be run locally (at ESPCI after the command \"conda activate simul\") or on google colab directly: \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Deyht/green_ai_espci/blob/main/green_ai_espci.ipynb)\n",
    "\n",
    "\n",
    "This notebook is **not intended as a standalone** document, the slides should be used as a reference to understand several of the explanations provided in the analysis of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b2f895",
   "metadata": {
    "id": "65b2f895"
   },
   "source": [
    "# Practical work 1: Optimization and HPC\n",
    "\n",
    "This first part tackles the subject of high-performance computing for the matrix multiplication operation, which is the **main operation in all modern AI models**.\n",
    "\n",
    "We can consider that the total energy consumed by a given computation can be approximated by $E = \\Delta P\\times T$, with $E$ the energy in Joules, $\\Delta P$ the increase in power draw compared to the system baseline in Watts, and $T$ the total time of the computation in seconds. We will first consider that $\\Delta P$ is constant so the consumed energy is simply proportional to the time of execution. Note that this is a strong simplification and that it completely neglects several aspects of the environmental impact of ICT and ignores possible rebound effects (see the slides for more details).\n",
    "\n",
    "Overall, the objective of this part will be to find the most efficient way of implementing the matrix multiplication operation and, therefore, reduce the amount of energy required.\n",
    "\n",
    "We will implement a classical matrix multiply operation between an $M\\times K$ matrix $A$ and a $K \\times N$ matrix $B$ to obtain an $M\\times N$ matrix $C$. The content of an element of $C$ is given by:\n",
    "\\begin{equation}\n",
    "C(i,j) = \\sum_k A(i,k)\\times B(k,j)\n",
    "\\end{equation}\n",
    "The indices $i$, $j$, and $k$ go through $M$, $N$, and $K$, respectively.\n",
    "\n",
    "\n",
    "In this part, we will compare different implementations of this operation and evaluate the impact of the programming language (interpreted with Python and compiled with C), low-level optimization, and parallelization. The example presented in this notebook were inspired and derived from [Algorithmica](https://en.algorithmica.org/hpc/algorithms/matmul/). We invite the reader to have a look at the chapter on matrix multiplication.\n",
    "\n",
    "*We note that we will mostly follow the triple loop algorithm and ignore possible alternatives like the Strassen algorithm as such alternative formulations are mostly interesting for large matrices and are more difficult to optimize for the memory hierarchy of numerical systems.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c8e8e",
   "metadata": {
    "id": "cc7c8e8e"
   },
   "source": [
    "**System informations**\n",
    "\n",
    "Before going further, you should gather information about the system you are running this notebook on (be it your laptop, a workstation, or a cloud service). Assuming you run on a Linux-based system, the following command will provide details about your system CPU. To interpret the results of this notebook, you should identify the number of cores/threads, the amount of L1/L2/L3 cache, and the supported instructions sets (AVX2). Some information might be missing, like the distribution of cores in P and E cores for Intel CPUs, but it can be found by searching the CPU model name online. To get a more detailed view of the memory hierachy of your system you can run the *lstopo* command (require the *hwloc* package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb09e87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 749
    },
    "id": "fcb09e87",
    "outputId": "08c64438-6590-474a-dabd-daf842cc7288"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.popen(\"lscpu\").read())\n",
    "\n",
    "\"\"\" #Uncomment block on GoogleColab for installing and running lstopo\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "os.system(\"apt install hwloc\")\n",
    "os.system(\"lstopo --of png > system_topography.png\")\n",
    "image = mpimg.imread(\"system_topography.png\")\n",
    "plt.figure(figsize=(6,4), dpi=200)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d4502",
   "metadata": {
    "id": "437d4502"
   },
   "source": [
    "## Python: Naive, compiled and built-in matrix multiplication\n",
    "\n",
    "### Matmul Naive: triple nested loop\n",
    "\n",
    "The following cell contains the most naive implementations of the matrix multiplication operation, **matmul_naive**, composed of three nested loops.\n",
    "\n",
    "We rely on the **time** library to measure its execution time. All computations are made using 32-bit (single precision) floating point variables arranged in numpy arrays. We also force numpy to use only one thread (for the moment) for a fair comparison with our non-parallelized custom functions.\n",
    "\n",
    "We start by establishing a performance baseline with $M=N=K=512$ for our matrices sizes.\n",
    "\n",
    "***Note:** Compute time predictions can vary due to other loads on the system. Ideally, you should run your compute time estimations a few times to average the variability.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88737810",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88737810",
    "outputId": "ab0c23db-e6ca-4ed8-967e-d92bc986dcec"
   },
   "outputs": [],
   "source": [
    "#Must cover all cases as numpy does not expose which version of BLAS it uses\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.system(\"export OMP_NUM_THREADS=1\")\n",
    "os.system(\"export OPENBLAS_NUM_THREADS=1\")\n",
    "os.system(\"export MKL_NUM_THREADS=1\")\n",
    "os.system(\"export NUMEXPR_NUM_THREADS=1\")\n",
    "import numpy as np\n",
    "import time as time\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def matmul_naive(A, B, C, M, N, K):\n",
    "    for i in range(0,M):\n",
    "        for j in range(0,N):\n",
    "            for k in range(0,K):\n",
    "                C[i,j] += A[i,k] * B[k,j]\n",
    "\n",
    "size = 512\n",
    "\n",
    "M = size; N = size; K = size\n",
    "\n",
    "A = (np.random.random((M,K)).astype(\"float32\")-0.5)*0.1\n",
    "B = (np.random.random((K,N)).astype(\"float32\")-0.5)-0.1\n",
    "C = np.zeros((M,N), dtype=\"float32\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "matmul_naive(A, B, C, M, N, K)\n",
    "\n",
    "matmul_naive_ref_time = time.time() - t_start\n",
    "\n",
    "print (\"matmul_naive compute time: %f s\"%(matmul_naive_ref_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c45d8",
   "metadata": {
    "id": "531c45d8"
   },
   "source": [
    "---\n",
    "### Matmul numpy inner sum\n",
    "\n",
    "In order to improve the performances, we can replace the inner loop of the previous function with an elment-wise product of two vectors and use **numpy** to sum the resulting elements. The use of vectorized operations should reduce the computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12406fa0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12406fa0",
    "outputId": "f42d1f71-1da0-4f3f-c9da-8491176d7614"
   },
   "outputs": [],
   "source": [
    "def matmul_numpy_sum(A, B, C, M, N, K):\n",
    "    for i in range(0,M):\n",
    "        for j in range(0,N):\n",
    "            C[i,j] = np.sum(A[i,:]*B[:,j])\n",
    "\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "matmul_numpy_sum(A, B, C, M, N, K)\n",
    "\n",
    "matmul_numpy_sum_time = time.time() - t_start\n",
    "\n",
    "print (\"matmul_numpy_sum time: %f s\"%(matmul_numpy_sum_time))\n",
    "print (\"Speedup over naive: %f\"%(matmul_naive_ref_time/matmul_numpy_sum_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aebfbe8",
   "metadata": {
    "id": "7aebfbe8"
   },
   "source": [
    "---\n",
    "### Numba compilation\n",
    "\n",
    "Python is an **interpreted** programming language, so conversion to machine language (compilation) is done on the fly on a per-line basis. However, some libraries like **numba** can be used to compile compatible Python functions in a C-like fashion.\n",
    "\n",
    "The following cell provides numba-compiled versions of the previous functions and evaluates the resulting speedup.\n",
    "\n",
    "***Note:** Functions with numba headers are compiled the first time they are called, taking time, and then kept in cache for future reuse. As a precaution, we always execute the function a first time before measuring its execution time.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a0afb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "506a0afb",
    "outputId": "24969c16-591c-4000-8409-b0498350c4e6"
   },
   "outputs": [],
   "source": [
    "from numba import jit #JIT stant for \"Just In Time\" compilation\n",
    "\n",
    "@jit(nopython=True, cache=True, fastmath=False)\n",
    "def compiled_matmul_naive(A, B, C, M, N, K):\n",
    "    for i in range(0,M):\n",
    "        for j in range(0,N):\n",
    "            for k in range(0,K):\n",
    "                C[i,j] += A[i,k] * B[k,j]\n",
    "\n",
    "@jit(nopython=True, cache=True, fastmath=False)\n",
    "def compiled_matmul_numpy_sum(A, B, C, M, N, K):\n",
    "    for i in range(0,M):\n",
    "        for j in range(0,N):\n",
    "            C[i,j] = np.sum(A[i,:]*B[:,j])\n",
    "\n",
    "compiled_matmul_naive(A, B, C, M, N, K)\n",
    "compiled_matmul_numpy_sum(A, B, C, M, N, K)\n",
    "\n",
    "t_start = time.time()\n",
    "compiled_matmul_naive(A, B, C, M, N, K)\n",
    "compiled_matmul_naive_time = time.time() - t_start\n",
    "\n",
    "t_start = time.time()\n",
    "compiled_matmul_numpy_sum(A, B, C, M, N, K)\n",
    "compiled_matmul_numpy_sum_time = time.time() - t_start\n",
    "\n",
    "print (\"compiled_naive time: %f s, and speedup compared to non-compiled: %f\"%\n",
    "    (compiled_matmul_naive_time, matmul_naive_ref_time/compiled_matmul_naive_time))\n",
    "print (\"compiled_sum time: %f s, and speedup compared to non-compiled: %f\"%\n",
    "    (compiled_matmul_numpy_sum_time, matmul_numpy_sum_time/compiled_matmul_numpy_sum_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a540286",
   "metadata": {
    "id": "7a540286"
   },
   "source": [
    "We see that the two compiled functions reach similar computation times, with a huge speedup for the naive version, while the speedup of the numpy_sum version speedup is relatively small.\n",
    "Most of the speed improvement we first observed with the non-compiled numpy_sum function comes from the fact that vectorized operations and most numpy functions rely on pre-compiled functions. Therefore, using numba to compile the matmul_numpy_sum only improve the two outer loops, while the most critical inner loop was already replaced by a compiled operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17261655",
   "metadata": {
    "id": "17261655"
   },
   "source": [
    "---\n",
    "### Performance curve\n",
    "\n",
    "To better represent the computing efficiency of different implementations, we can estimate the number of floating-point operations per second (FLOPS). This can be done by dividing the total number of \"useful\" operations by the total time of computation.\n",
    "\n",
    "For matrix multiplication where $M=N=K$ the number of operations is simply $N^3$. This scaling law shows that the compute time will increase exponentially with the size of the matrices.\n",
    "\n",
    "The following cells measure the compute time as a function of the problem size for the compiled naive implementation and draw the corresponding performance curve.\n",
    "\n",
    "You can try to adapt the size-step in order to better catch the moment where the total size of all matrices in bytes ($ 3*N^2*4$ for simple precision floating points (4 bytes) and considering $M=N=K$) is larger than the amount of L3 cache in your system. You can also try to change the function for the compiled numpy_sum version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b091f6e",
   "metadata": {
    "id": "5b091f6e"
   },
   "outputs": [],
   "source": [
    "\n",
    "def perf_curve_fct(c_fct, start=128, end=1920, size_step=128):\n",
    "\n",
    "    nb_steps = (end-start+size_step)//size_step\n",
    "    gflops_per_size = np.zeros((nb_steps,2))\n",
    "\n",
    "    M = start; N = start; K = start\n",
    "\n",
    "    A = (np.random.random((M,K)).astype(\"float32\")-0.5)*0.1\n",
    "    B = (np.random.random((K,N)).astype(\"float32\")-0.5)-0.1\n",
    "    C = np.zeros((M,N), dtype=\"float32\")\n",
    "\n",
    "    #first non-measured call in case it need to be recompiled\n",
    "    c_fct(A, B, C, M, N, K)\n",
    "\n",
    "    for i in range(0,nb_steps):\n",
    "\n",
    "        l_size = start + i*size_step\n",
    "        M = l_size; N = l_size; K = l_size\n",
    "\n",
    "        A = (np.random.random((M,K)).astype(\"float32\")-0.5)*0.1\n",
    "        B = (np.random.random((K,N)).astype(\"float32\")-0.5)-0.1\n",
    "        C = np.zeros((M,N), dtype=\"float32\")\n",
    "\n",
    "        t_start = time.time()\n",
    "        c_fct(A, B, C, M, N, K)\n",
    "\n",
    "        elapsed_time = time.time() - t_start\n",
    "\n",
    "        gflops_per_size[i,0] = l_size\n",
    "        gflops_per_size[i,1] = l_size**3/1e9/elapsed_time\n",
    "\n",
    "    return gflops_per_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa2dc8",
   "metadata": {
    "id": "9bfa2dc8"
   },
   "outputs": [],
   "source": [
    "#Actual computation of the curve, might take some time\n",
    "compiled_naive_gflops_curve = perf_curve_fct(compiled_matmul_naive, start=64, end=1920, size_step=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba16ebf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "ba16ebf2",
    "outputId": "90bd678d-1e58-4308-e2e3-c37ef6a8d46b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(compiled_naive_gflops_curve[:,0], compiled_naive_gflops_curve[:,1])\n",
    "plt.gca().set_xlabel(\"Size N\")\n",
    "plt.gca().set_ylabel(\"GFLOPS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55b5f0",
   "metadata": {
    "id": "1f55b5f0"
   },
   "source": [
    "In theory, this curve should highlight that the performances (GLFOPS) first increase with the size of the matrices until they cannot fit in the L3 CPU cache anymore. After this point, the performances go back down with the problem size and exhibit a relatively undeterministic/noisy behavior. This implementation is currently strongly bandwidth-limited with some added latency specific to python for large problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe231a",
   "metadata": {
    "id": "30fe231a"
   },
   "source": [
    "---\n",
    "### Built-in python and numpy matmul\n",
    "\n",
    "We now compare our compiled naive implementation with build-in and optimized implementations available in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d6c31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "353d6c31",
    "outputId": "e8a02df4-8724-4b8c-edd1-6b504cb4fd07"
   },
   "outputs": [],
   "source": [
    "size = 1024 #Larger than before since all functions are quite fast\n",
    "\n",
    "M = size; N = size; K = size\n",
    "\n",
    "A = (np.random.random((M,K)).astype(\"float32\")-0.5)*0.1\n",
    "B = (np.random.random((K,N)).astype(\"float32\")-0.5)-0.1\n",
    "C = np.zeros((M,N), dtype=\"float32\")\n",
    "\n",
    "\n",
    "compiled_matmul_naive(A, B, C, M, N, K)\n",
    "t_start = time.time()\n",
    "compiled_matmul_naive(A, B, C, M, N, K)\n",
    "compiled_matmul_naive_time = time.time() - t_start\n",
    "\n",
    "t_start = time.time()\n",
    "C = A@B\n",
    "at_operator_time = time.time() - t_start\n",
    "\n",
    "t_start = time.time()\n",
    "C = np.matmul(A,B)\n",
    "numpy_builtin_matmul_time = time.time() - t_start\n",
    "\n",
    "\n",
    "print (\"Compiled_naive time: %f s\"%(compiled_matmul_naive_time))\n",
    "print (\"@ operator time: %f s\"%(at_operator_time))\n",
    "print (\"Numpy builtin matmul time: %f s\"%(numpy_builtin_matmul_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d4393",
   "metadata": {
    "id": "8b9d4393"
   },
   "outputs": [],
   "source": [
    "def numpy_builtin_matmul_wrapper(A, B, C, M, N, K):\n",
    "    C = np.matmul(A,B)\n",
    "    \n",
    "#Actual computation of the curve, might take some time\n",
    "numpy_builtin_matmul_gflops_curve = perf_curve_fct(numpy_builtin_matmul_wrapper, start=128, end=3072, size_step=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e7639",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "527e7639",
    "outputId": "b302ea30-44a2-4a98-a7a5-82b0e1f8bde1"
   },
   "outputs": [],
   "source": [
    "plt.plot(compiled_naive_gflops_curve[:,0], compiled_naive_gflops_curve[:,1], label=\"Compiled Naiv\")\n",
    "plt.plot(numpy_builtin_matmul_gflops_curve[:,0], numpy_builtin_matmul_gflops_curve[:,1], label=\"Numpy Built-in\")\n",
    "\n",
    "plt.gca().set_xlabel(\"Size N\")\n",
    "plt.gca().set_ylabel(\"GFLOPS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2e772",
   "metadata": {
    "id": "3db2e772"
   },
   "source": [
    "We can see that the performance of the built-in numpy matmul function is not only much higher than our custom implementation (even with compilation) but also maintained for large problem sizes, which indicates that it is likely compute-limited and not bandwidth-limited anymore.\n",
    "\n",
    "The secret is that it uses the highly optimized openBLAS C library in the background (see numpy [linalg](https://numpy.org/doc/2.1/reference/routines.linalg.html#module-numpy.linalg)). The very purpose of the following part will be to explain how we can achieve such a level of performance in C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aade7e1",
   "metadata": {
    "id": "4aade7e1"
   },
   "source": [
    "## C: Optimized low-level matrix multiplication\n",
    "\n",
    "In this part, we will analyze multiple implementations of the matrix multiplication operation in C with increasing levels of optimization and complexity.\n",
    "\n",
    "While the C programming language supports multi-dimensional array indexing (but no vectorized operations), most implementations (and low-level C libraries) opt for a flattened matrix formalism. This choice more explicitly exposes the actual arrangement of the data in the system memory and allows us to identify possible cache-miss effects more easily. Therefore, we adopt a column-major indexing as it is the one used in most low-level linear algebra libraries (inherited from Fortran implementations).\n",
    "For a matrix with $M$ rows and $N$ columns, $i$ and $j$ indexing the rows and columns respectively, the $C(i,j)$ element of the flat matrix encoded in column-major can be accessed with $C[j\\times M+i]$.\n",
    "\n",
    "In this part, we will again measure performances over square matrices with $M=N=K$. Still all the implementations will be given for arbitrary shaped matrices, exposing the specificities over the $M$, $N$ and $K$ dimensions.\n",
    "\n",
    "In order to manage all the codes from the notebook, we will write our C functions in cells that save their content into a .c file. Then, we will launch the compilation and execution instructions from python cells.\n",
    "\n",
    "The following cell creates an auxiliary file that contains helper functions that we will use with all our matmul implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbffe29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cbffe29",
    "outputId": "a91a0a22-ca82-419f-e4d6-a134e94c5782"
   },
   "outputs": [],
   "source": [
    "%%writefile aux.c\n",
    "\n",
    "#include \"aux.h\"\n",
    "\n",
    "void init_timing(struct timeval* tstart)\n",
    "{\n",
    "    gettimeofday(tstart, NULL);\n",
    "}\n",
    "\n",
    "float elapsed_time(struct timeval tstart)\n",
    "{\n",
    "    struct timeval tmp;\n",
    "    long long diff;\n",
    "    gettimeofday(&tmp, NULL);\n",
    "    diff = tmp.tv_usec - tstart.tv_usec;\n",
    "    diff += (tmp.tv_sec - tstart.tv_sec) * 1000000;\n",
    "    return ((float)diff*1.0e-6);\n",
    "}\n",
    "\n",
    "void create_matrices(float** A, float** B, float** C, int M, int N, int K)\n",
    "{\n",
    "    size_t i;\n",
    "\n",
    "    *A = (float*) calloc(M*K,sizeof(float));\n",
    "    *B = (float*) calloc(K*N,sizeof(float));\n",
    "    *C = (float*) calloc(M*N,sizeof(float));\n",
    "\n",
    "    for(i = 0; i < M*K; i++)\n",
    "        (*A)[i] = ((float)rand()/RAND_MAX-0.5)*0.1;\n",
    "\n",
    "    for(i = 0; i < K*N; i++)\n",
    "        (*B)[i] = ((float)rand()/RAND_MAX-0.5)*0.1;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8aafe0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c8aafe0",
    "outputId": "5b0a7737-452a-4e4b-ccb4-9315c36cd613"
   },
   "outputs": [],
   "source": [
    "%%writefile aux.h\n",
    "\n",
    "#ifndef AUX_H\n",
    "#define AUX_H\n",
    "\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <cblas.h>\n",
    "#include <tgmath.h>\n",
    "\n",
    "#include <sys/time.h>\n",
    "\n",
    "void init_timing(struct timeval* tstart);\n",
    "float elapsed_time(struct timeval tstart);\n",
    "void create_matrices(float** A, float** B, float** C, int M, int N, int K);\n",
    "\n",
    "#endif //AUX_H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e8b55",
   "metadata": {
    "id": "238e8b55"
   },
   "source": [
    "---\n",
    "### OpenBLAS GEMM baseline\n",
    "\n",
    "Our objective is to get compute times that are as close as possible to the OpenBLAS matrix multiplication SGEMM function. So, we start by getting the performances of SGEMM. Since this implementation is very efficient we will estimate its compute time and GFLOPS directly for a large 1920 square matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300d200",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9300d200",
    "outputId": "d71a9666-fea3-4447-fc28-8f6aa542fd71"
   },
   "outputs": [],
   "source": [
    "%%writefile matmul_blas.c\n",
    "\n",
    "#include \"aux.h\"\n",
    "struct timeval timer;\n",
    "\n",
    "void matmul_blas(const float* A, const float* B, float* C, int M, int N, int K)\n",
    "{\n",
    "    float alpha = 1.0f, beta = 0.0f;\n",
    "\n",
    "    cblas_sgemm(CblasColMajor, CblasNoTrans, CblasNoTrans,\n",
    "        M, N, K, alpha, A, M, B, K, beta, C, M);\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    size_t l_size = atoi(argv[1]);\n",
    "    float *A, *B, *C;\n",
    "\n",
    "    create_matrices(&A, &B, &C, l_size, l_size, l_size);\n",
    "\n",
    "    init_timing(&timer);\n",
    "    matmul_blas(A, B, C, l_size, l_size, l_size);\n",
    "    printf(\"%f\", elapsed_time(timer));\n",
    "\n",
    "    exit(EXIT_SUCCESS);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32393f5",
   "metadata": {
    "id": "a32393f5"
   },
   "source": [
    "In this first part we want to measure the performances without parallelization, so we set the number of threads that OpenBLAS can use to 1.\n",
    "\n",
    "C being a compiled language, we need to call the gcc compiler and link the openblas library every time we modify the source code. This can be done with:\n",
    "```sh\n",
    "gcc aux.h aux.c matmul_blas.c -o matmul_blas -lopenblas\n",
    "```\n",
    "This line creates an executable file called matmul_blas.\n",
    "\n",
    "The C code has been defined so the size of the matrix is red from the command line after the name of the executable. The only output of the C code is the elapsed time in second. The next cell shows how we can compile, execute for a given matrix size, and read the result of the C code from a python cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa648bac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa648bac",
    "outputId": "7ffc3bdb-6354-4f8c-d474-35b8e9cdb130"
   },
   "outputs": [],
   "source": [
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.system(\"gcc aux.h aux.c matmul_blas.c -o matmul_blas -lm -lopenblas\")\n",
    "\n",
    "l_size = 1920\n",
    "\n",
    "elapsed_time_sgemm = float(os.popen(\"./matmul_blas %d\"%(l_size)).read())\n",
    "gflops_sgemm = l_size**3/1e9/elapsed_time_sgemm\n",
    "print (\"SGEMM BLAS at size %d \\t time %f s \\t GFLOPS %f\"%(l_size, elapsed_time_sgemm, gflops_sgemm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79455879",
   "metadata": {
    "id": "79455879"
   },
   "source": [
    "---\n",
    "### Matmul_v1: Naive triple nested loop\n",
    "\n",
    "Our first hand-made implementation follows the same naive approach of the three nested loops. In contrast with python, such construction is common in C and is already strongly optimized through compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30bf30e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c30bf30e",
    "outputId": "5d5baa4a-2442-4029-e480-8ff547a30109"
   },
   "outputs": [],
   "source": [
    "%%writefile matmul_v1.c\n",
    "\n",
    "#include \"aux.h\"\n",
    "struct timeval timer;\n",
    "\n",
    "//Naive triple loop matmul\n",
    "void matmul_v1(const float *A, const float *B, float *C, int M, int N, int K)\n",
    "{\n",
    "    int i,j,k;\n",
    "    double acc;\n",
    "\n",
    "    for(j = 0; j < N; j++)\n",
    "        for(i = 0; i < M; i++)\n",
    "            for(k = 0; k < K; k++)\n",
    "                C[j*M+i] += A[k*M+i] * B[j*K+k];\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    size_t l_size = atoi(argv[1]);\n",
    "    float *A, *B, *C;\n",
    "\n",
    "    create_matrices(&A, &B, &C, l_size, l_size, l_size);\n",
    "\n",
    "    init_timing(&timer);\n",
    "    matmul_v1(A, B, C, l_size, l_size, l_size);\n",
    "    printf(\"%f\", elapsed_time(timer));\n",
    "\n",
    "    exit(EXIT_SUCCESS);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d9704",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d8d9704",
    "outputId": "07e9ab01-770d-42e7-a0b9-99b2ed8864da"
   },
   "outputs": [],
   "source": [
    "#In case starting directly with the C part\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.system(\"gcc aux.h aux.c matmul_v1.c -o matmul_v1 -lopenblas -lm\")\n",
    "\n",
    "l_size = 512\n",
    "elapsed_time_matmul_v1 = float(os.popen(\"./matmul_v1 %d\"%(l_size)).read())\n",
    "gflops_matmul_v1 = l_size**3/1e9/elapsed_time_matmul_v1\n",
    "print (\"matmul_v1 at size %d \\t time %f s \\t GFLOPS %f\"%(l_size, elapsed_time_matmul_v1, gflops_matmul_v1))\n",
    "\n",
    "l_size = 1920\n",
    "\n",
    "elapsed_time_matmul_v1 = float(os.popen(\"./matmul_v1 %d\"%(l_size)).read())\n",
    "gflops_matmul_v1 = l_size**3/1e9/elapsed_time_matmul_v1\n",
    "print (\"matmul_v1 at size %d \\t time %f s \\t GFLOPS %f\"%(l_size, elapsed_time_matmul_v1, gflops_matmul_v1))\n",
    "\n",
    "print (\"Performances as a fraction of OpenBLAS SGEMM: %.2f %%\"%((gflops_matmul_v1/gflops_sgemm)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c56eba",
   "metadata": {
    "id": "22c56eba"
   },
   "source": [
    "This implementation achieves the same performance level as our python compiled-naive implementation for an identical matrix size. Still, it corresponds to less than 1% of the peak performances reached by OpenBLAS SGEMM at the same size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463d777",
   "metadata": {
    "id": "6463d777"
   },
   "source": [
    "For now, we are using the compiler's default configuration. While this default behavior depends on the exact compiler used (e.g., intel compiler is more aggressive than gcc by default), most have a relatively \"safe\" behavior with the majority of available optimizations turned off.\n",
    "\n",
    "The following cell measures the change in computation time of our matmul_v1 function when compiled with different levels of optimization flags (-O1 to -O3, see [GCC Optimize options](https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html) for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96047786",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96047786",
    "outputId": "4dbcf320-03a7-48f0-fe5a-ca962ac52650"
   },
   "outputs": [],
   "source": [
    "l_size = 1920\n",
    "\n",
    "print(\"All computations done with size %d\"%(l_size))\n",
    "\n",
    "opt_list = [\"-O1\", \"-O2\", \"-O3\"]\n",
    "\n",
    "for c_opt in opt_list:\n",
    "\n",
    "    os.system(\"gcc aux.h aux.c matmul_v1.c -o matmul_v1 -lopenblas -lm %s\"%(c_opt))\n",
    "    elapsed_time_matmul_v1 = float(os.popen(\"./matmul_v1 %d\"%(l_size)).read())\n",
    "    gflops_matmul_v1 = l_size**3/1e9/elapsed_time_matmul_v1\n",
    "    print (\"matmul_v1 %s \\t time %f s \\t GFLOPS %f\"%(c_opt, elapsed_time_matmul_v1, gflops_matmul_v1))\n",
    "\n",
    "print (\"Performances of v1 as a fraction of OpenBLAS SGEMM: %.2f %%\"%((gflops_matmul_v1/gflops_sgemm)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8d4679",
   "metadata": {
    "id": "cb8d4679"
   },
   "source": [
    "Depending on the test environment, you should observe that either -O1 or -O2 is enough to maximize performances, while -O3 only provides a marginal improvement or and can even slightly re-increase computation time compared to -O2.\n",
    "\n",
    "We can now create the performance curve for this function to identify its limiting behavior.\n",
    "\n",
    "*Note: In another environment (AMD CPU) we even observed that adding any optimization flags can reduce the performances with this implementation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad029f",
   "metadata": {
    "id": "39ad029f"
   },
   "outputs": [],
   "source": [
    "def perf_curve_fct_C(line_fct, start=128, end=1920, size_step=128):\n",
    "\n",
    "    nb_steps = (end-start+size_step)//size_step\n",
    "    gflops_per_size = np.zeros((nb_steps,2))\n",
    "\n",
    "    for i in range(0,nb_steps):\n",
    "\n",
    "        l_size = start + i*size_step\n",
    "\n",
    "        elapsed_time = float(os.popen(\"%s %d\"%(line_fct, l_size)).read())\n",
    "\n",
    "        gflops_per_size[i,0] = l_size\n",
    "        gflops_per_size[i,1] = l_size**3/1e9/elapsed_time\n",
    "\n",
    "    return gflops_per_size\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.system(\"gcc -O2 aux.h aux.c matmul_v1.c -o matmul_v1 -lopenblas -lm\")\n",
    "\n",
    "#Actual computation of the curve, might take some time\n",
    "matmul_v1_gflops_curve = perf_curve_fct_C(\"./matmul_v1\", start=64, end=1920, size_step=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeff253",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "ebeff253",
    "outputId": "d49f759c-19f3-48ea-fe97-706e751f9dcf"
   },
   "outputs": [],
   "source": [
    "plt.plot(compiled_naive_gflops_curve[:,0], compiled_naive_gflops_curve[:,1], label=\"Python Numba-naive\")\n",
    "plt.plot(matmul_v1_gflops_curve[:,0], matmul_v1_gflops_curve[:,1], label=\"C matmul_V1\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Size N\")\n",
    "plt.gca().set_ylabel(\"GFLOPS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410d778",
   "metadata": {
    "id": "8410d778"
   },
   "source": [
    "The performance curve of matmul_v1 is very similar to the one from our Python compiled Naive version and exhibits the same behavior, indicating that the function is bandwidth-limited (default in the memory management)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3157c",
   "metadata": {
    "id": "fef3157c"
   },
   "source": [
    "---\n",
    "\n",
    "### Matmul_v2: Inner loop accumulator\n",
    "\n",
    "In order to lift the bandwidth limit, we need to improve our use of the system's memory hierarchy. In other words, we need to reduce and mutualize access to global memory and reuse data as much as possible as possible once they are loaded from the RAM to the different levels of CPU cache and CPU register.\n",
    "\n",
    "One minimalistic change to our matmul_v1 function is to add a local accumulator for the inner loop sum and write the result to the C matrix only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8ff0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ad8ff0c",
    "outputId": "8f9e86cd-3ca1-487c-8013-50223a209f52"
   },
   "outputs": [],
   "source": [
    "%%writefile matmul_v2.c\n",
    "\n",
    "#include \"aux.h\"\n",
    "struct timeval timer;\n",
    "\n",
    "//v1 + simple register accumulate\n",
    "void matmul_v2(const float *A, const float *B, float *C, int M, int N, int K)\n",
    "{\n",
    "    int i,j,k;\n",
    "    float acc;\n",
    "\n",
    "    for(j = 0; j < N; j++)\n",
    "        for(i = 0; i < M; i++)\n",
    "        {\n",
    "            acc = 0.0;\n",
    "            for(k = 0; k < K; k++)\n",
    "                acc += A[k*M+i] * B[j*K+k];\n",
    "            C[j*M+i] = acc;\n",
    "        }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    size_t l_size = atoi(argv[1]);\n",
    "    float *A, *B, *C;\n",
    "\n",
    "    create_matrices(&A, &B, &C, l_size, l_size, l_size);\n",
    "\n",
    "    init_timing(&timer);\n",
    "    matmul_v2(A, B, C, l_size, l_size, l_size);\n",
    "    printf(\"%f\", elapsed_time(timer));\n",
    "\n",
    "    exit(EXIT_SUCCESS);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffe7a7c",
   "metadata": {
    "id": "9ffe7a7c"
   },
   "source": [
    "We evaluate the compute times and GFLOPS of this matmul_v2 implementation for all possible compilation optimization levels (None, -O1, -O2, -O3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6a090",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dd6a090",
    "outputId": "ac41adaf-9e5d-42c2-a1d0-a48f1282bf97"
   },
   "outputs": [],
   "source": [
    "l_size = 1920\n",
    "\n",
    "print(\"All computations done with size %d\"%(l_size))\n",
    "\n",
    "opt_list = [\"\", \"-O1\", \"-O2\", \"-O3\"]\n",
    "\n",
    "for c_opt in opt_list:\n",
    "\n",
    "    os.system(\"gcc aux.h aux.c matmul_v2.c -o matmul_v2 -lopenblas -lm %s\"%(c_opt))\n",
    "    elapsed_time_matmul_v2 = float(os.popen(\"./matmul_v2 %d\"%(l_size)).read())\n",
    "    gflops_matmul_v2 = l_size**3/1e9/elapsed_time_matmul_v2\n",
    "    print (\"matmul_v2 %s \\t time %f s \\t GFLOPS %f\"%(c_opt, elapsed_time_matmul_v2, gflops_matmul_v2))\n",
    "\n",
    "print (\"Performances of v2 as a fraction of OpenBLAS SGEMM: %.2f %%\"%((gflops_matmul_v2/gflops_sgemm)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c4205",
   "metadata": {
    "id": "ca1c4205"
   },
   "source": [
    "Compared to matmul_v1, the performances are improved when no compilation flag is used, but it should make no major difference after -O1. This indicates that the compiler with -Ox is able to identify redundant updates of an element in the global memory and, by itself, uses a temporary variable for the accumulation and does the update only once.\n",
    "\n",
    "As before, we plot the performance curve for matmul_v2 to observe that there is no major difference in the function behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895e922",
   "metadata": {
    "id": "0895e922"
   },
   "outputs": [],
   "source": [
    "os.system(\"gcc -O2 aux.h aux.c matmul_v2.c -o matmul_v2 -lopenblas -lm\")\n",
    "\n",
    "#Actual computation of the curve, might take some time\n",
    "matmul_v2_gflops_curve = perf_curve_fct_C(\"./matmul_v2\", start=64, end=1920, size_step=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5923a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "89a5923a",
    "outputId": "3dbb3413-8611-465d-a383-72b64904de7c"
   },
   "outputs": [],
   "source": [
    "plt.plot(compiled_naive_gflops_curve[:,0], compiled_naive_gflops_curve[:,1], label=\"Python Numba-naive\")\n",
    "plt.plot(matmul_v1_gflops_curve[:,0], matmul_v1_gflops_curve[:,1], label=\"C matmul_V1\")\n",
    "plt.plot(matmul_v2_gflops_curve[:,0], matmul_v2_gflops_curve[:,1], label=\"C matmul_V2\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Size N\")\n",
    "plt.gca().set_ylabel(\"GFLOPS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258f1fd",
   "metadata": {
    "id": "2258f1fd"
   },
   "source": [
    "---\n",
    "### Matmul_v3: Transposed matrix and auto-vectorization\n",
    "\n",
    "From a memory bandwidth perspective, the main limiting aspect of our matmul v1 and v2 is that the required elements of one of the two matrices, A and B, are not continuous in the system memory.  \n",
    "In the inner loop, k addresses the elements of the $i$-th row of $A$ and the elements of the $j$-th column of $B$. With our column-major convention, elements of $B$ are continuous in memory but not the elements of $A$, creating a cache-miss in the memory hierarchy.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Deyht/green_ai_espci/refs/heads/main/images/memory_hierarchy_illustration.png\" width=\"700\">\n",
    "\n",
    "One solution is then to transpose the matrix $A$ before computing the matrix multiplication. While performing the transposition adds time and increases memory usage, it allows to restore memory continuity for both $A$ and $B$ matrices. We implement this approach as matmul_v3 in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ab238",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "718ab238",
    "outputId": "fa205bf0-1bd3-4d46-c30b-e6f983fce5f1"
   },
   "outputs": [],
   "source": [
    "%%writefile matmul_v3.c\n",
    "\n",
    "#include \"aux.h\"\n",
    "struct timeval timer;\n",
    "\n",
    "//v2 + transposed A\n",
    "//With the proper set of compilation optimization flags\n",
    "//this function will be auto-vectorized\n",
    "void matmul_v3(const float *A, const float *B, float *C, int M, int N, int K)\n",
    "{\n",
    "    int i,j,k;\n",
    "    float *t_A = (float*) malloc(M*K*sizeof(float));\n",
    "    float acc;\n",
    "\n",
    "    for(i = 0; i < M; i++)\n",
    "        for(k = 0; k < K; k++)\n",
    "            t_A[i*K+k] = A[k*M+i];\n",
    "\n",
    "\n",
    "    for(j = 0; j < N; j++)\n",
    "        for(i = 0; i < M; i++)\n",
    "        {\n",
    "            acc = 0.0;\n",
    "            for(k = 0; k < K; k++)\n",
    "                acc += t_A[i*K+k] * B[j*K+k];\n",
    "            C[j*M+i] = acc;\n",
    "        }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    size_t l_size = atoi(argv[1]);\n",
    "    float *A, *B, *C;\n",
    "\n",
    "    create_matrices(&A, &B, &C, l_size, l_size, l_size);\n",
    "\n",
    "    init_timing(&timer);\n",
    "    matmul_v3(A, B, C, l_size, l_size, l_size);\n",
    "    printf(\"%f\", elapsed_time(timer));\n",
    "\n",
    "    exit(EXIT_SUCCESS);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4e30c",
   "metadata": {
    "id": "4eb4e30c"
   },
   "source": [
    "As before we test the different levels of -Ox optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468a10c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6468a10c",
    "outputId": "04722471-ce86-4f11-8d8e-d7d572ef41ef"
   },
   "outputs": [],
   "source": [
    "l_size = 1920\n",
    "\n",
    "print(\"All computations done with size %d\"%(l_size))\n",
    "\n",
    "opt_list = [\"\", \"-O1\", \"-O2\", \"-O3\"]\n",
    "\n",
    "for c_opt in opt_list:\n",
    "\n",
    "    os.system(\"gcc aux.h aux.c matmul_v3.c -o matmul_v3 -lopenblas -lm %s\"%(c_opt))\n",
    "    elapsed_time_matmul_v3 = float(os.popen(\"./matmul_v3 %d\"%(l_size)).read())\n",
    "    gflops_matmul_v3 = l_size**3/1e9/elapsed_time_matmul_v3\n",
    "    print (\"matmul_v3 %s \\t time %f s \\t GFLOPS %f\"%(c_opt, elapsed_time_matmul_v3, gflops_matmul_v3))\n",
    "\n",
    "print (\"Performances of v3 as a fraction of OpenBLAS SGEMM: %.2f %%\"%((gflops_matmul_v3/gflops_sgemm)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e340474",
   "metadata": {
    "id": "7e340474"
   },
   "source": [
    "This version results in a significant speedup compared to the previous two, even without compile optimization.\n",
    "\n",
    "In addition, there is a strong improvement once any optimization flag is added. This is the result of the compiler identifying that we perform element-wise multiplications of two vectors of data that are continuous in memory. The compiler then chose to express these operations as vectorized SIMD instructions based on the instruction sets supported by the CPU. This behavior is called auto-vectorization.\n",
    "\n",
    "We now try to explicitly set some additional optimization flags to increase the performance even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01c166",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d01c166",
    "outputId": "d77baaeb-8e00-4cb7-ffc4-698244248daa"
   },
   "outputs": [],
   "source": [
    "l_size = 1920\n",
    "\n",
    "print(\"All computations done with size %d\"%(l_size))\n",
    "\n",
    "base_opt = \"-O3\"\n",
    "#Depending on your system and gcc version, best performances might be achieved with either -O2 or -O3\n",
    "\n",
    "opt_list = [\"-march=native \", \"-funroll-loops \", \"-ffast-math \",\n",
    "           \"-march=native -funroll-loops\",\n",
    "           \"-ffast-math -funroll-loops\",\n",
    "           \"-ffast-math -march=native\",\n",
    "           \"-ffast-math -march=native -funroll-loops\"]\n",
    "\n",
    "for c_opt in opt_list:\n",
    "\n",
    "    os.system(\"gcc aux.h aux.c matmul_v3.c -o matmul_v3 -lopenblas -lm %s %s\"%(base_opt, c_opt))\n",
    "    elapsed_time_matmul_v3 = float(os.popen(\"./matmul_v3 %d\"%(l_size)).read())\n",
    "    gflops_matmul_v3 = l_size**3/1e9/elapsed_time_matmul_v3\n",
    "    print (\"matmul_v3 %s %-44s time %.4f s \\t GFLOPS %f\"%(base_opt, c_opt, elapsed_time_matmul_v3, gflops_matmul_v3))\n",
    "\n",
    "print (\"Performances of v3 as a fraction of OpenBLAS SGEMM: %.2f %%\"%((gflops_matmul_v3/gflops_sgemm)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c96da",
   "metadata": {
    "id": "954c96da"
   },
   "source": [
    "While the combination of the three options should grant the best performance, it is strongly dependent on the system. In our testing environment, we reached 16.6% of OpenBLAS SGEMM performances with this version. In the next cell, we trace the performance curve using our best compilation setup, which must be adapted for your own system based on the results from the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4616345",
   "metadata": {
    "id": "c4616345"
   },
   "outputs": [],
   "source": [
    "os.system(\"gcc aux.h aux.c matmul_v3.c -o matmul_v3 -lopenblas -lm -O3 -ffast-math -march=native -funroll-loops\")\n",
    "\n",
    "#Actual computation of the curve, might take some time\n",
    "matmul_v3_gflops_curve = perf_curve_fct_C(\"./matmul_v3\", start=64, end=1920, size_step=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ed7b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "906ed7b3",
    "outputId": "57898a48-840d-4f91-a3ed-4f69cb4d4570"
   },
   "outputs": [],
   "source": [
    "plt.plot(compiled_naive_gflops_curve[:,0], compiled_naive_gflops_curve[:,1], label=\"Python Numba-naive\")\n",
    "plt.plot(matmul_v1_gflops_curve[:,0], matmul_v1_gflops_curve[:,1], label=\"C matmul_V1\")\n",
    "plt.plot(matmul_v2_gflops_curve[:,0], matmul_v2_gflops_curve[:,1], label=\"C matmul_V2\")\n",
    "plt.plot(matmul_v3_gflops_curve[:,0], matmul_v3_gflops_curve[:,1], label=\"C matmul_V3\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Size N\")\n",
    "plt.gca().set_ylabel(\"GFLOPS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb68145",
   "metadata": {
    "id": "5eb68145"
   },
   "source": [
    "The performance curve of matmul_v3 is much higher than those of previous versions. It is also much more stable, but it still loses performance for larger problem sizes. This is again a sign of bandwidth limitation, but it is no longer caused by a cache miss from L3 to RAM. To gain more performance, we will need to start optimizing CPU cache usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab5fbb",
   "metadata": {
    "id": "b5ab5fbb"
   },
   "source": [
    "### Matmul_v4: Manual vectorization using GCC SIMD build-in vector support\n",
    "\n",
    "\n",
    "While auto-vectorization is a powerful feature of the compiler, we will see how we can match the performances of matmul_v3 with a manual vectorized implementation that makes use of built-in gcc vector data structures that allow abstract SIMD operations using the AVX2 instruction set. For example, the following line declares a type vector composed of 32 Bytes (or 256 Bits, which is the default vector size for AVX2), which can store 8 simple precision floating point values of 4 Bytes/32 bits each.\n",
    "```\n",
    "typedef float vec __attribute__ ((vector_size(32));\n",
    "```\n",
    "While it is possible to invoke the vector multiplication instruction manually, when doing operations on vector data, the compiler will automatically use the corresponding vectorized FMA (Fused Multiply Add) operation.\n",
    "\n",
    "While this specific implementation would not necessarily be portable to another compiler, equivalent data types are available to reproduce the same implementation principles. Overall, this type of low-level optimization starts to be hardware-specific (brand of CPU, specific architecture, etc).\n",
    "\n",
    "The following cell implements matmul_v4 that creates vectorized versions of $A^T$ and $B$ and fills them with the corresponding data. The element-wise products are performed vector-wise using an implicit vectorized FMA instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6777f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baa6777f",
    "outputId": "375bdcde-f459-40dc-cc20-937ca0363b13"
   },
   "outputs": [],
   "source": [
    "%%writefile matmul_v4.c\n",
    "\n",
    "#include \"aux.h\"\n",
    "struct timeval timer;\n",
    "\n",
    "//v3 + manual vectorization through memory aligned SIMD operations\n",
    "//Note, require C11 ISO, not compatible with C99 while auto vectorization is compatible\n",
    "\n",
    "// Define a memory aligned vector of 8 floats (32 bytes wide vector, ie 256 bits)\n",
    "typedef float vec __attribute__ (( vector_size(32) ));\n",
    "\n",
    "void matmul_v4(const float *A, const float *B, float *C, int M, int N, int K)\n",
    "{\n",
    "    int i,j,k;\n",
    "    // Rounded number of 8-element vec in a K\n",
    "    if(K % 8 != 0)\n",
    "    {\n",
    "        printf(\"Error, mismatch between matrix size and kernel size\");\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    int n_K = K/8;\n",
    "\n",
    "\n",
    "    vec *a = (vec*) aligned_alloc(32,M*n_K*32);\n",
    "    vec *b = (vec*) aligned_alloc(32,N*n_K*32);\n",
    "\n",
    "    for(i = 0; i < M; i++)\n",
    "        for(j = 0; j < K; j++)\n",
    "            a[i*n_K + j/8][j%8] = A[j*M+i];\n",
    "\n",
    "    for(i = 0; i < N; i++)\n",
    "        for(j = 0; j < K; j++)\n",
    "            b[i*n_K + j/8][j%8] = B[i*K+j];\n",
    "\n",
    "\n",
    "    for(j = 0; j < N; j++)\n",
    "        for(i = 0; i < M; i++)\n",
    "        {\n",
    "            vec acc = {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f};\n",
    "            for(k = 0; k < n_K; k++)\n",
    "                acc += a[i*n_K+k] * b[j*n_K+k];\n",
    "\n",
    "            for(k = 0; k < 8; k++)\n",
    "                C[j*M+i] += acc[k];\n",
    "        }\n",
    "}\n",
    "\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    size_t l_size = atoi(argv[1]);\n",
    "    float *A, *B, *C;\n",
    "\n",
    "    create_matrices(&A, &B, &C, l_size, l_size, l_size);\n",
    "\n",
    "    init_timing(&timer);\n",
    "    matmul_v4(A, B, C, l_size, l_size, l_size);\n",
    "    printf(\"%f\", elapsed_time(timer));\n",
    "\n",
    "    exit(EXIT_SUCCESS);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c64477",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99c64477",
    "outputId": "577e3976-316c-4510-fab6-0537d72db8fb"
   },
   "outputs": [],
   "source": [
    "l_size = 1920\n",
    "\n",
    "print(\"All computations done with size %d\"%(l_size))\n",
    "\n",
    "opt_list = [\"\", \"-O1\", \"-O2\", \"-O3\"]\n",
    "\n",
    "for c_opt in opt_list:\n",
    "\n",
    "    os.system(\"gcc aux.h aux.c matmul_v4.c -o matmul_v4 -lopenblas -lm %s\"%(c_opt))\n",
    "    elapsed_time_matmul_v4 = float(os.popen(\"./matmul_v4 %d\"%(l_size)).read())\n",
    "    gflops_matmul_v4 = l_size**3/1e9/elapsed_time_matmul_v4\n",
    "    print (\"matmul_v4 %s \\t time %f s \\t GFLOPS %f\"%(c_opt, elapsed_time_matmul_v4, gflops_matmul_v4))\n",
    "\n",
    "print (\"Performances of v4 as a fraction of OpenBLAS SGEMM: %.2f %%\"%((gflops_matmul_v4/gflops_sgemm)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa6c9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4fa6c9a",
    "outputId": "60c9f013-26c0-48a6-96e8-3e6498f681ff"
   },
   "outputs": [],
   "source": [
    "l_size = 1920\n",
    "\n",
    "os.system(\"gcc aux.h aux.c matmul_v4.c -o matmul_v4 -lopenblas -lm -O3 -ffast-math -march=native -funroll-loops\")\n",
    "elapsed_time_matmul_v4 = float(os.popen(\"./matmul_v4 %d\"%(l_size)).read())\n",
    "gflops_matmul_v4 = l_size**3/1e9/elapsed_time_matmul_v4\n",
    "print (\"matmul_v4 \\t time %f s \\t GFLOPS %f\"%(elapsed_time_matmul_v4, gflops_matmul_v4))\n",
    "\n",
    "print (\"Performances of v4 as a fraction of OpenBLAS SGEMM: %.2f %%\"%((gflops_matmul_v4/gflops_sgemm)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65093779",
   "metadata": {
    "id": "65093779"
   },
   "source": [
    "Compared to matmul_v3 we doubled the GFLOPS, still using only -O2 and -O3, without advanced compilation options. With the advanced options, matmul_v3 and matmul_v4 achieve the same performances. The reasons are detailed after the computation of the performance curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd67cc",
   "metadata": {
    "id": "02dd67cc"
   },
   "outputs": [],
   "source": [
    "os.system(\"gcc aux.h aux.c matmul_v4.c -o matmul_v4 -lopenblas -lm -O3 -ffast-math -march=native -funroll-loops\")\n",
    "\n",
    "#Actual computation of the curve, might take some time\n",
    "matmul_v4_gflops_curve = perf_curve_fct_C(\"./matmul_v4\", start=64, end=1920, size_step=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526a512",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "d526a512",
    "outputId": "690d24aa-a93b-4f5b-f111-1f9805fc2641"
   },
   "outputs": [],
   "source": [
    "plt.plot(compiled_naive_gflops_curve[:,0], compiled_naive_gflops_curve[:,1], label=\"Python Numba-naive\")\n",
    "plt.plot(matmul_v1_gflops_curve[:,0], matmul_v1_gflops_curve[:,1], label=\"C matmul_V1\")\n",
    "plt.plot(matmul_v2_gflops_curve[:,0], matmul_v2_gflops_curve[:,1], label=\"C matmul_V2\")\n",
    "plt.plot(matmul_v3_gflops_curve[:,0], matmul_v3_gflops_curve[:,1], label=\"C matmul_V3\")\n",
    "plt.plot(matmul_v4_gflops_curve[:,0], matmul_v4_gflops_curve[:,1], label=\"C matmul_V4\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Size N\")\n",
    "plt.gca().set_ylabel(\"GFLOPS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a687d2b3",
   "metadata": {
    "id": "a687d2b3"
   },
   "source": [
    "As expected, matmul_v4 and matmul_v3 have the same performance curve. This means that we have likely manually reproduced the auto-vectorization behavior from the compiler when using all the optimization flags.\n",
    "\n",
    "The comparison between matmul_v4 and matmul_v3 serves two objectives:\n",
    "- Illustrate that it is possible to reach high performances with relatively naive implementations without low-level optimization by relying on the compiler.\n",
    "- Serves as an example of how vectorized data types work, considering they are necessary to reach even greater performances through low-level CPU cache optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0069c02",
   "metadata": {
    "id": "d0069c02"
   },
   "source": [
    "---\n",
    "### Matmul_v5: optimum data reuse with in-register vectorized kernel\n",
    "\n",
    "We will now shift our approach to the problem. Instead of thinking from the RAM to the cache, we will try to identify how we can fit the core operations of our matmul algorithm inside the fastest cache level, namely the CPU registry. We will then scale up by optimizing the loading of the CPU register from progressively slower level of CPU caches.\n",
    "\n",
    "In the previous approach, we fixed a column of $B$ (most outer loop) to optimize its reuse and then loop (intermediate loop) over the rows of the transposed version of A. However, with this approach, it is obvious that we need to reload the rows of $A$ as many times as we have columns of $B$, leading to multiple loadings of the same data. In addition, as soon as either one row or one column is larger than a given level of cache, we need multiple copies over this cache every time we reload the data.\n",
    "\n",
    "In contrast, the core of the new implementation is a low-level kernel that will process a $k_h$ set of rows from $A$ and a $k_w$ set of columns from $B$ and compute all the cross products and sums to fill the corresponding sub-part of $C$. The kernel will then take the $k_h$ elements from the first column of the subset of $A$ and the $k_w$ elements from the first row of the subset of $B$. The kernel then computes the element-wise product of these two vectors and adds the result in the appropriate elements of $C$. \n",
    "\n",
    "<img src=\"https://raw.github.com/Deyht/green_ai_espci/refs/heads/main/images/kernel_v5_illustration.png\" width=\"800\">\n",
    "\n",
    "With this approach, we have effectively considered all the computations in which the loaded elements are involved without the need to load them again later. Now, to properly fill the $C$ matrix, we need to repeat this operation over the $K$ dimension for all columns of $A$ and rows of $B$ in the selected subset to accumulate the products of all the required elements. Once the sub-part of the $C$ matrix is filled with the final value, we can repeat this approach for another subset of rows from $A$ and columns from $B$.\n",
    "\n",
    "However, all this strategy would be useless if individual vectors remain too large to fit in the fastest level of cache. Our objective is to run the kernel inside the CPU register. On most modern CPUs that support AVX instructions, the register can store up to 16 vector data structures of 4 Bytes each. In addition, FMA instructions can run partially in parallel, meaning that we need to compute on a minimum number of registers to saturate computing performances. Here, we estimate that parallel FMA instructions on 10 registers are enough to saturate the execution port. Considering that we need two vectors for intermediate computations, we wille build a kernel logic that makes use of $2\\times6=12$ vectors (the size of the $C$ sub-matrix and subset of rows and columns for $A$ and $B$, respectively) for a total of 14 vectors. We consider vectors aligned with the elements in memory, meaning that our kernel will work on an area corresponding to $k_h=2\\times8=16$ rows of $A$ and $k_w=6$ columns of $B$.\n",
    "\n",
    "For each position $k$, the kernel must contain two loops:  \n",
    "1) A loop over the 6 width kernel element of matrix B. At each step, the current value must be broadcasted to a vector with 8 identical values.  \n",
    "2) A loop over the the height kernel element of matrix A. This time the 16 elements are decomposed over 2 vectors.  \n",
    "3) The two current vectors can be multiplied to accumulate 8 products in the kernel corresponding to C[i:i+8,j] += A[i:i+8] * B[j]\n",
    "\n",
    "Finally, this kernel must be called in a double loop for all its possible positions in C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596a45f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0596a45f",
    "outputId": "c729ef10-d24d-4796-b7ef-f14bb523df0f"
   },
   "outputs": [],
   "source": [
    "%%writefile matmul_v5.c\n",
    "\n",
    "#include \"aux.h\"\n",
    "struct timeval timer;\n",
    "\n",
    "//Matmul using FMA operation on a sub-part of C and optimizing the cache usage\n",
    "//No need for transposition in this version as most of the work is done in cache anyway\n",
    "\n",
    "// Define a memory aligned vector of 8 floats (32 bytes wide vector, ie 256 bits)\n",
    "typedef float vec __attribute__ (( vector_size(32) ));\n",
    "\n",
    "#define ker_w 6\n",
    "#define ker_h 16\n",
    "//ker_h must be a multiple of 8\n",
    "\n",
    "void kernel(const float *_a, const float *_b, float *_c,\n",
    "    int M, int N, int K, int i, int j, int k_start, int k_stop)\n",
    "{\n",
    "    int k, b_m, b_n, l;\n",
    "    float val;\n",
    "    //declared in CPU register\n",
    "    vec t[ker_h/8][ker_w] = {};\n",
    "\n",
    "    for(k = k_start; k < k_stop; k++)\n",
    "    {\n",
    "        for(b_n = 0; b_n < ker_w; b_n++)\n",
    "        {\n",
    "            //brodcast B value to the full vector\n",
    "            val = _b[(i+b_n)*K+k];\n",
    "            vec beta = {val, val, val, val, val, val, val, val};\n",
    "\n",
    "            for(b_m = 0; b_m < ker_h/8; b_m++)\n",
    "            {\n",
    "                vec alpha;\n",
    "                for(l = 0; l < 8; l++)\n",
    "                    alpha[l] = _a[j+k*M+b_m*8+l];\n",
    "                t[b_m][b_n] += alpha * beta; // converts to an fma\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // write the results back to C\n",
    "    for(b_n = 0; b_n < ker_w; b_n++)\n",
    "        for(b_m = 0; b_m < ker_h/8; b_m++)\n",
    "            for(l = 0; l < 8; l++)\n",
    "                _c[j+(i+b_n)*M+b_m*8+l] += t[b_m][b_n][l];\n",
    "}\n",
    "\n",
    "\n",
    "void matmul_v5(const float *A, const float *B, float *C, int M, int N, int K)\n",
    "{\n",
    "    int i,j,k;\n",
    "\n",
    "    if(M % ker_h != 0 || N % ker_w != 0)\n",
    "    {\n",
    "        printf(\"Error, mismatch between matrix size and kernel size\");\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "\n",
    "    for(j = 0; j < M; j += ker_h)\n",
    "        for(i = 0; i < N; i += ker_w)\n",
    "            kernel(A, B, C, M, N, K, i, j, 0, K);\n",
    "}\n",
    "\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    size_t l_size = atoi(argv[1]);\n",
    "    float *A, *B, *C;\n",
    "\n",
    "    create_matrices(&A, &B, &C, l_size, l_size, l_size);\n",
    "\n",
    "    init_timing(&timer);\n",
    "    matmul_v5(A, B, C, l_size, l_size, l_size);\n",
    "    printf(\"%f\", elapsed_time(timer));\n",
    "\n",
    "    exit(EXIT_SUCCESS);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfd7ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17dfd7ed",
    "outputId": "8883c424-92e7-4b79-ed3e-88ebe81f1625"
   },
   "outputs": [],
   "source": [
    "l_size = 48*40 #Must be a multiple of 48 for the kernel to work\n",
    "\n",
    "print(\"All computations done with size %d\"%(l_size))\n",
    "\n",
    "opt_list = [\"\", \"-O1\", \"-O2\", \"-O3\"]\n",
    "\n",
    "for c_opt in opt_list:\n",
    "\n",
    "    os.system(\"gcc aux.h aux.c matmul_v5.c -o matmul_v5 -lopenblas -lm %s\"%(c_opt))\n",
    "    elapsed_time_matmul_v5 = float(os.popen(\"./matmul_v5 %d\"%(l_size)).read())\n",
    "    gflops_matmul_v5 = l_size**3/1e9/elapsed_time_matmul_v5\n",
    "    print (\"matmul_v5 %s \\t time %f s \\t GFLOPS %f\"%(c_opt, elapsed_time_matmul_v5, gflops_matmul_v5))\n",
    "\n",
    "print (\"Performances of v5 as a fraction of OpenBLAS SGEMM: %.2f %%\"%((gflops_matmul_v5/gflops_sgemm)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ba846",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd2ba846",
    "outputId": "fa582f3b-a1a7-4c79-87b0-3c88b3fd9170"
   },
   "outputs": [],
   "source": [
    "l_size = 48*40\n",
    "\n",
    "os.system(\"gcc aux.h aux.c matmul_v5.c -o matmul_v5 -lopenblas -lm -O3 -ffast-math -march=native -funroll-loops\")\n",
    "elapsed_time_matmul_v5 = float(os.popen(\"./matmul_v5 %d\"%(l_size)).read())\n",
    "gflops_matmul_v5 = l_size**3/1e9/elapsed_time_matmul_v5\n",
    "print (\"matmul_v5 \\t time %f s \\t GFLOPS %f\"%(elapsed_time_matmul_v5, gflops_matmul_v5))\n",
    "\n",
    "print (\"Performances of v5 as a fraction of OpenBLAS SGEMM: %.2f %%\"%((gflops_matmul_v5/gflops_sgemm)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bdc3cf",
   "metadata": {
    "id": "12bdc3cf"
   },
   "outputs": [],
   "source": [
    "os.system(\"gcc aux.h aux.c matmul_v5.c -o matmul_v5 -lopenblas -lm -O3 -ffast-math -march=native -funroll-loops\")\n",
    "\n",
    "#Actual computation of the curve, might take some time\n",
    "matmul_v5_gflops_curve = perf_curve_fct_C(\"./matmul_v5\", start=96, end=3072, size_step=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c48fbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "85c48fbd",
    "outputId": "2446e1e0-cbaf-4223-dbe2-f37e7bd0d526"
   },
   "outputs": [],
   "source": [
    "plt.plot(compiled_naive_gflops_curve[:,0], compiled_naive_gflops_curve[:,1], label=\"Python Numba-naive\")\n",
    "plt.plot(matmul_v1_gflops_curve[:,0], matmul_v1_gflops_curve[:,1], label=\"C matmul_V1\")\n",
    "plt.plot(matmul_v2_gflops_curve[:,0], matmul_v2_gflops_curve[:,1], label=\"C matmul_V2\")\n",
    "plt.plot(matmul_v3_gflops_curve[:,0], matmul_v3_gflops_curve[:,1], label=\"C matmul_V3\")\n",
    "plt.plot(matmul_v4_gflops_curve[:,0], matmul_v4_gflops_curve[:,1], label=\"C matmul_V4\")\n",
    "plt.plot(matmul_v5_gflops_curve[:,0], matmul_v5_gflops_curve[:,1], label=\"C matmul_V5\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Size N\")\n",
    "plt.gca().set_ylabel(\"GFLOPS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997f2a8",
   "metadata": {
    "id": "d997f2a8"
   },
   "source": [
    "---\n",
    "### Matmul_v6: Blocked matrix multiplication with register kernel\n",
    "\n",
    "The previous version is still bandwidth limited. To further optimize we must maximize the reuse of data loaded in the different CPU caches by working on blocks of the matrices.\n",
    "\n",
    "Our biggest cache miss is the frequent change of columns in $A$ through the loop over $K$. Our fastest L1 cache must contain as much columns of $A$ as possible. This will also define the number of lines in $B$. We defined $BDimK$ the number of columns of $A$.\n",
    "\n",
    "Our second cache miss is for the change in columns in $B$ through the loop over N. This one will mostly define our L2 cache. We define $BDimN$ as the number of columns from B.\n",
    "\n",
    "Finally, we want to cache rows of $A$ in a way that fill the L3 cache considering the two previous dimensions. We define $BDimN$ as the number of row from $A$.\n",
    "\n",
    "Each block must be a multiple of the kernel size, so a loop over all the kernel position in the block can be used.\n",
    "\n",
    "<img src=\"https://raw.github.com/Deyht/green_ai_espci/refs/heads/main/images/blocked_matmul_v6_illustration.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59076e66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59076e66",
    "outputId": "b017eda4-572a-4d3e-f4a6-0b13e1325655"
   },
   "outputs": [],
   "source": [
    "%%writefile matmul_v6.c\n",
    "\n",
    "#include \"aux.h\"\n",
    "struct timeval timer;\n",
    "\n",
    "//Matmul using FMA operation on a sub-part of C and optimizing the cache usage\n",
    "//No need for transposition in this version as most of the work is done in cache anyway\n",
    "\n",
    "// Define a memory aligned vector of 8 floats (32 bytes wide vector, ie 256 bits)\n",
    "typedef float vec __attribute__ (( vector_size(32) ));\n",
    "\n",
    "#define ker_w 6\n",
    "#define ker_h 16\n",
    "//ker_h must be a multiple of 8\n",
    "\n",
    "\n",
    "void kernel(const float *_a, const float *_b, float *_c,\n",
    "    int M, int N, int K, int i, int j, int k_start, int k_stop)\n",
    "{\n",
    "    int k, b_m, b_n, l;\n",
    "    float val;\n",
    "    //declared in CPU register\n",
    "    vec t[ker_h/8][ker_w] = {};\n",
    "\n",
    "    for(k = k_start; k < k_stop; k++)\n",
    "    {\n",
    "        for(b_n = 0; b_n < ker_w; b_n++)\n",
    "        {\n",
    "            //brodcast B value to the full vector\n",
    "            val = _b[(i+b_n)*K+k];\n",
    "            vec beta = {val, val, val, val, val, val, val, val};\n",
    "\n",
    "            for(b_m = 0; b_m < ker_h/8; b_m++)\n",
    "            {\n",
    "                vec alpha;\n",
    "                for(l = 0; l < 8; l++)\n",
    "                    alpha[l] = _a[j+k*M+b_m*8+l];\n",
    "                t[b_m][b_n] += alpha * beta; // converts to an fma\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // write the results back to C\n",
    "    for(b_n = 0; b_n < ker_w; b_n++)\n",
    "        for(b_m = 0; b_m < ker_h/8; b_m++)\n",
    "            for(l = 0; l < 8; l++)\n",
    "                _c[j+(i+b_n)*M+b_m*8+l] += t[b_m][b_n][l];\n",
    "}\n",
    "\n",
    "\n",
    "//V4 but with blocking decomposition to successively load sub parts of A and B\n",
    "//into L2 and L1 cache for maximum reuse. Maximizing the achievable memory bandwidth\n",
    "\n",
    "void matmul_v6(const float *A, const float *B, float *C, int M, int N, int K)\n",
    "{\n",
    "    int i,j,k;\n",
    "\n",
    "    const int l3 = 64; //Number of rows from A\n",
    "    const int l2 = 120; //Number of columns from B\n",
    "    const int l1 = 120; //Number of columns from A\n",
    "\n",
    "    if(M % ker_h != 0 || N % ker_w != 0 || l2 % ker_w != 0 || l3 % ker_h != 0)\n",
    "    {\n",
    "        printf(\"Error, mismatch between matrix size and kernel size\");\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    int i1, i2, i3;\n",
    "\n",
    "    for(i3 = 0; i3 < M; i3 += l3)\n",
    "        for(i2 = 0; i2 < N; i2 += l2)\n",
    "            for(i1 = 0; i1 < K; i1 += l1)\n",
    "                for(j = i3; j < fmin(i3+l3,M); j += ker_h)\n",
    "                    for(i = i2; i < fmin(i2+l2,N); i += ker_w)\n",
    "                        kernel(A, B, C, M, N, K, i, j, i1, fmin(i1+l1,K));\n",
    "}\n",
    "\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    size_t l_size = atoi(argv[1]);\n",
    "    float *A, *B, *C;\n",
    "\n",
    "    create_matrices(&A, &B, &C, l_size, l_size, l_size);\n",
    "\n",
    "    init_timing(&timer);\n",
    "    matmul_v6(A, B, C, l_size, l_size, l_size);\n",
    "    printf(\"%f\", elapsed_time(timer));\n",
    "\n",
    "    exit(EXIT_SUCCESS);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bac80b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2bac80b",
    "outputId": "cac1d14b-7fa9-4e0f-8857-4c318a98dfbe"
   },
   "outputs": [],
   "source": [
    "l_size = 48*40 #Must be a multiple of 48 for the kernel to work\n",
    "\n",
    "print(\"All computations done with size %d\"%(l_size))\n",
    "\n",
    "opt_list = [\"\", \"-O1\", \"-O2\", \"-O3\"]\n",
    "\n",
    "for c_opt in opt_list:\n",
    "\n",
    "    os.system(\"gcc aux.h aux.c matmul_v6.c -o matmul_v6 -lopenblas -lm %s\"%(c_opt))\n",
    "    elapsed_time_matmul_v6 = float(os.popen(\"./matmul_v6 %d\"%(l_size)).read())\n",
    "    gflops_matmul_v6 = l_size**3/1e9/elapsed_time_matmul_v6\n",
    "    print (\"matmul_v6 %s \\t time %f s \\t GFLOPS %f\"%(c_opt, elapsed_time_matmul_v6, gflops_matmul_v6))\n",
    "\n",
    "print (\"Performances of v6 as a fraction of OpenBLAS SGEMM: %.2f %%\"%((gflops_matmul_v6/gflops_sgemm)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede2b09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ede2b09",
    "outputId": "0526376b-5225-46bc-dce3-3dd7bc49d850"
   },
   "outputs": [],
   "source": [
    "l_size = 48*40\n",
    "\n",
    "os.system(\"gcc aux.h aux.c matmul_v6.c -o matmul_v6 -lopenblas -lm -O3 -ffast-math -march=native -funroll-loops\")\n",
    "elapsed_time_matmul_v6 = float(os.popen(\"./matmul_v6 %d\"%(l_size)).read())\n",
    "gflops_matmul_v6 = l_size**3/1e9/elapsed_time_matmul_v6\n",
    "print (\"matmul_v6 \\t time %f s \\t GFLOPS %f\"%(elapsed_time_matmul_v6, gflops_matmul_v6))\n",
    "\n",
    "print (\"Performances of v6 as a fraction of OpenBLAS SGEMM: %.2f %%\"%((gflops_matmul_v6/gflops_sgemm)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261efa5",
   "metadata": {
    "id": "5261efa5"
   },
   "outputs": [],
   "source": [
    "os.system(\"gcc aux.h aux.c matmul_v6.c -o matmul_v6 -lopenblas -lm -O3 -ffast-math -march=native -funroll-loops\")\n",
    "\n",
    "#Actual computation of the curve, might take some time\n",
    "matmul_v6_gflops_curve = perf_curve_fct_C(\"./matmul_v6\", start=96, end=3072, size_step=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1b360",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "70b1b360",
    "outputId": "07a1b9ed-abce-4f1c-9461-58c82a827d0d"
   },
   "outputs": [],
   "source": [
    "plt.plot(compiled_naive_gflops_curve[:,0], compiled_naive_gflops_curve[:,1], label=\"Python Numba-naive\")\n",
    "plt.plot(matmul_v1_gflops_curve[:,0], matmul_v1_gflops_curve[:,1], label=\"C matmul_V1\")\n",
    "plt.plot(matmul_v2_gflops_curve[:,0], matmul_v2_gflops_curve[:,1], label=\"C matmul_V2\")\n",
    "plt.plot(matmul_v3_gflops_curve[:,0], matmul_v3_gflops_curve[:,1], label=\"C matmul_V3\")\n",
    "plt.plot(matmul_v4_gflops_curve[:,0], matmul_v4_gflops_curve[:,1], label=\"C matmul_V4\")\n",
    "plt.plot(matmul_v5_gflops_curve[:,0], matmul_v5_gflops_curve[:,1], label=\"C matmul_V5\")\n",
    "plt.plot(matmul_v6_gflops_curve[:,0], matmul_v6_gflops_curve[:,1], label=\"C matmul_V6\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Size N\")\n",
    "plt.gca().set_ylabel(\"GFLOPS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13a184",
   "metadata": {
    "id": "8f13a184"
   },
   "source": [
    "This time, we are very close to a performance curve that maintains its performances over a wide range of matrix sizes, indicating that we likely reached a regime of computing limitation. Now, we can plot the actual performance curve for the OpenBLAS SGEMM version for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0013a3",
   "metadata": {
    "id": "0b0013a3"
   },
   "outputs": [],
   "source": [
    "sgemm_gflops_curve = perf_curve_fct_C(\"./matmul_blas\", start=96, end=3072, size_step=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64442df1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "64442df1",
    "outputId": "26a66c69-a044-42e9-d179-094a1176c870"
   },
   "outputs": [],
   "source": [
    "plt.plot(compiled_naive_gflops_curve[:,0], compiled_naive_gflops_curve[:,1], label=\"Python Numba-naive\")\n",
    "plt.plot(matmul_v1_gflops_curve[:,0], matmul_v1_gflops_curve[:,1], label=\"C matmul_V1\")\n",
    "plt.plot(matmul_v2_gflops_curve[:,0], matmul_v2_gflops_curve[:,1], label=\"C matmul_V2\")\n",
    "plt.plot(matmul_v3_gflops_curve[:,0], matmul_v3_gflops_curve[:,1], label=\"C matmul_V3\")\n",
    "plt.plot(matmul_v4_gflops_curve[:,0], matmul_v4_gflops_curve[:,1], label=\"C matmul_V4\")\n",
    "plt.plot(matmul_v5_gflops_curve[:,0], matmul_v5_gflops_curve[:,1], label=\"C matmul_V5\")\n",
    "plt.plot(matmul_v6_gflops_curve[:,0], matmul_v6_gflops_curve[:,1], label=\"C matmul_V6\")\n",
    "plt.plot(sgemm_gflops_curve[:,0], sgemm_gflops_curve[:,1], label=\"OpenBLAS SGEMM\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Size N\")\n",
    "plt.gca().set_ylabel(\"GFLOPS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea495f",
   "metadata": {
    "id": "e6ea495f"
   },
   "source": [
    "Bridging the remaining gap between matmul_v6 and the OpenBLAS version of SGEMM would require several small adjustments like rewriting the kernel without any loops (listing all operations explicitly), ensure memorry alignement, or having specific kernel and block sizes depending on the global matrices sizes. OpenBLAS does exactly size with various pre-compiled versions of optimized matrix multiplication that are selected depending on the context and problem shape, all hidden behind a single call to SGEMM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40b313",
   "metadata": {
    "id": "8a40b313"
   },
   "source": [
    "---\n",
    "\n",
    "## OpenMP parallelization\n",
    "\n",
    "In this part we evaluate how we modify our matmul_v6 imlplementation so it is parallelized of multiple threads using OpenMP. First, we start by evaluating the scaling of OpenBLAS version for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b0bab",
   "metadata": {
    "id": "b27b0bab"
   },
   "outputs": [],
   "source": [
    "#Must be set to 2 if runing in colab\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"6\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"6\"\n",
    "import numpy as np\n",
    "\n",
    "para_sgemm_gflops_curve = perf_curve_fct_C(\"./matmul_blas\", start=96, end=3072, size_step=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101f491e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "101f491e",
    "outputId": "537cde09-e0ce-433d-c924-65b0c0eb8b4a"
   },
   "outputs": [],
   "source": [
    "plt.plot(sgemm_gflops_curve[:,0], sgemm_gflops_curve[:,1], c=\"C0\", ls=\"--\", label=\"OpenBLAS SGEMM\")\n",
    "plt.plot(para_sgemm_gflops_curve[:,0], para_sgemm_gflops_curve[:,1], c=\"C0\", label=\"OpenBLAS SGEMM para\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Size N\")\n",
    "plt.gca().set_ylabel(\"GFLOPS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984d5b7",
   "metadata": {},
   "source": [
    "OpenMP allow to parallelize the execution of code on shared memory systems. For a detailed introduction of OpenMP capabilities, we recommand reading Idris course on [OpenMP](http://www.idris.fr/media/formations/openmp/idris_openmp_cours-v2.11.pdf). In summary, OpenMP wille rely on the master thread of the program to create parallel zones. All the functionalities of OpenMP are accessible through \"pre-processing\" directives that are automatically interpreted during compilation and replace with actual lines of codes and instructions. For the programme, this formalism allows to easily compile a program with or without the OpenMP directives taken into acount by the compiler.\n",
    "\n",
    "One of the simplest way of using OpenMP to accelerate a computation is to use a loop where operations are independant and distribute its iterations over multiple threads/cores. Here we can follow this principle to parallelize over the most outer loop of our blocked matmul implementation.\n",
    "\n",
    "The number of threads in use is defined by a environmental variable (OMP_NUM_THREADS), so it is not necessary to recompile in order to change the number of threads.\n",
    "\n",
    "<img src=\"https://raw.github.com/Deyht/green_ai_espci/refs/heads/main/images/openmp_loop_example.png\" align=\"left\" width=\"400\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d92d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "743d92d3",
    "outputId": "241d390f-f60f-4363-9007-80e5272f4c2c"
   },
   "outputs": [],
   "source": [
    "%%writefile matmul_v6_para.c\n",
    "\n",
    "#include \"aux.h\"\n",
    "struct timeval timer;\n",
    "\n",
    "//Matmul using FMA operation on a sub-part of C and optimizing the cache usage\n",
    "//No need for transposition in this version as most of the work is done in cache anyway\n",
    "\n",
    "// Define a memory aligned vector type of 8 floats (32 bit)\n",
    "typedef float vec __attribute__ (( vector_size(32) ));\n",
    "\n",
    "#define ker_w 6\n",
    "#define ker_h 16\n",
    "//ker_h must be a multiple of 8\n",
    "\n",
    "void kernel(const float *_a, const float *_b, float *_c,\n",
    "    int M, int N, int K, int i, int j, int k_start, int k_stop)\n",
    "{\n",
    "    int k, b_m, b_n, l;\n",
    "    float val;\n",
    "    //declared in CPU register\n",
    "    vec t[ker_h/8][ker_w] = {};\n",
    "\n",
    "    for(k = k_start; k < k_stop; k++)\n",
    "    {\n",
    "        for(b_n = 0; b_n < ker_w; b_n++)\n",
    "        {\n",
    "            //brodcast B value to the full vector\n",
    "            val = _b[(i+b_n)*K+k];\n",
    "            vec beta = {val, val, val, val, val, val, val, val};\n",
    "\n",
    "            for(b_m = 0; b_m < ker_h/8; b_m++)\n",
    "            {\n",
    "                vec alpha;\n",
    "                for(l = 0; l < 8; l++)\n",
    "                    alpha[l] = _a[j+k*M+b_m*8+l];\n",
    "                t[b_m][b_n] += alpha * beta; // converts to an fma\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // write the results back to C\n",
    "    for(b_n = 0; b_n < ker_w; b_n++)\n",
    "        for(b_m = 0; b_m < ker_h/8; b_m++)\n",
    "            for(l = 0; l < 8; l++)\n",
    "                _c[j+(i+b_n)*M+b_m*8+l] += t[b_m][b_n][l];\n",
    "}\n",
    "\n",
    "\n",
    "//V4 but with blocking decomposition to successively load sub parts of A and B\n",
    "//into L2 and L1 cache for maximum reuse. Maximizing the achievable memory bandwidth\n",
    "\n",
    "void matmul_v6(const float *A, const float *B, float *C, int M, int N, int K)\n",
    "{\n",
    "    int i,j,k;\n",
    "\n",
    "    const int l3 = 64; //Number of rows from A\n",
    "    const int l2 = 120; //Number of columns from B\n",
    "    const int l1 = 240; //Number of columns from A\n",
    "\n",
    "    if(M % ker_h != 0 || N % ker_w != 0 || l2 % ker_w != 0 || l3 % ker_h != 0)\n",
    "    {\n",
    "        printf(\"Error, mismatch between matrix size and kernel size\");\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    int i1, i2, i3;\n",
    "\n",
    "    #pragma omp parallel for schedule(dynamic, 1) private(i3,i2,i1,i,j)\n",
    "    for(i3 = 0; i3 < M; i3 += l3)\n",
    "        for(i2 = 0; i2 < N; i2 += l2)\n",
    "            for(i1 = 0; i1 < K; i1 += l1)\n",
    "                for(j = i3; j < fmin(i3+l3,M); j += ker_h)\n",
    "                    for(i = i2; i < fmin(i2+l2,N); i += ker_w)\n",
    "                        kernel(A, B, C, M, N, K, i, j, i1, fmin(i1+l1,K));\n",
    "}\n",
    "\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    size_t l_size = atoi(argv[1]);\n",
    "    float *A, *B, *C;\n",
    "\n",
    "    create_matrices(&A, &B, &C, l_size, l_size, l_size);\n",
    "\n",
    "    init_timing(&timer);\n",
    "    matmul_v6(A, B, C, l_size, l_size, l_size);\n",
    "    printf(\"%f\", elapsed_time(timer));\n",
    "\n",
    "    exit(EXIT_SUCCESS);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fccf78f",
   "metadata": {},
   "source": [
    "To take into account OpenMP directives (otherwise ignore by the pre-processor) we must add -fopenmp to the compilation line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a0386",
   "metadata": {
    "id": "999a0386"
   },
   "outputs": [],
   "source": [
    "os.system(\"gcc aux.h aux.c matmul_v6_para.c -o matmul_v6_para -fopenmp -lopenblas -lm -O3 -ffast-math -march=native -funroll-loops\")\n",
    "\n",
    "#Actual computation of the curve, might take some time\n",
    "para_matmul_v6_gflops_curve = perf_curve_fct_C(\"./matmul_v6_para\", start=96, end=3072, size_step=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b725483e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "b725483e",
    "outputId": "ccc8e578-cdca-4ccf-fed5-39ff4b81a370"
   },
   "outputs": [],
   "source": [
    "plt.plot(sgemm_gflops_curve[:,0], sgemm_gflops_curve[:,1], c=\"C0\", ls=\"--\", label=\"OpenBLAS SGEMM\")\n",
    "plt.plot(para_sgemm_gflops_curve[:,0], para_sgemm_gflops_curve[:,1], c=\"C0\", label=\"OpenBLAS SGEMM para\")\n",
    "plt.plot(matmul_v6_gflops_curve[:,0], matmul_v6_gflops_curve[:,1], c=\"C1\", ls=\"--\", label=\"C matmul_V6\")\n",
    "plt.plot(para_matmul_v6_gflops_curve[:,0], para_matmul_v6_gflops_curve[:,1], c=\"C1\", label=\"matmul_v6_para\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"Size N\")\n",
    "plt.gca().set_ylabel(\"GFLOPS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589495be",
   "metadata": {},
   "source": [
    "We see that our implementation scales properly when using less threads than phisical cores on our system, which is another indication that we properly use the memory hierarchy, especially considering that L2 and L1 levels of cache are core specific. A secondary effect of parallelization is that it breaks the global problem into smaller pieces, which is why we usually don't need more advanced implementation of the matrix multiplication algorithme as the sub-probrlem handled by a single core is most likely too small for it to positively impact performances.\n",
    "\n",
    "Now we test how both our matmu_v6 and OpenBLAS scales by computing the speedup as a function of the number of threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7076316",
   "metadata": {
    "id": "a7076316"
   },
   "outputs": [],
   "source": [
    "def para_scaling_curve_fct_C(line_fct, l_size, nb_threads):\n",
    "\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "    time_ref = float(os.popen(\"%s %d\"%(line_fct, l_size)).read())\n",
    "\n",
    "    speedup_curve = np.zeros((nb_threads,2))\n",
    "    speedup_curve[0,:] = 1\n",
    "\n",
    "    for n_th in range(2,nb_threads+1):\n",
    "\n",
    "        os.environ[\"OMP_NUM_THREADS\"] = \"%d\"%(n_th)\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"%d\"%(n_th)\n",
    "\n",
    "        elapsed_time = float(os.popen(\"%s %d\"%(line_fct, l_size)).read())\n",
    "\n",
    "        speedup_curve[n_th-1,0] = n_th\n",
    "        speedup_curve[n_th-1,1] = time_ref/elapsed_time\n",
    "\n",
    "    return speedup_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cf207",
   "metadata": {
    "id": "841cf207"
   },
   "outputs": [],
   "source": [
    "para_matmul_v6_speedup_curve = para_scaling_curve_fct_C(\"./matmul_v6_para\", 1920, 12)\n",
    "para_sgemm_speedup_curve = para_scaling_curve_fct_C(\"./matmul_blas\", 1920, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26acdfb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "26acdfb9",
    "outputId": "82296697-3b0f-4098-bbd8-13915b69a5e6"
   },
   "outputs": [],
   "source": [
    "plt.plot(para_sgemm_speedup_curve[:,0], para_sgemm_speedup_curve[:,1], c=\"C0\", label=\"OpenBLAS SGEMM para\")\n",
    "plt.plot(para_matmul_v6_speedup_curve[:,0], para_matmul_v6_speedup_curve[:,1], c=\"C1\", label=\"matmul_v6_para\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"N of CPU threads\")\n",
    "plt.gca().set_ylabel(\"Speedup\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18123433",
   "metadata": {},
   "source": [
    "Depending on your test system, you might observe that OpenBLAS starts loosing more performances than our implementation when the number of threads get larger than the number of physical performance cores. After this point new threads might either be treated by hyperthreading of by E cores if your CPU has a Big/Little architectures (intel). Both can strongly impact OpenBLAS performances as many low-level optmization are done to reach the optimum level of performances. These optimizations might not work well when two threads are on the same physical core, or when the code is executed on a core with a different architecture that the default P cores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36bb7f9",
   "metadata": {
    "id": "f36bb7f9"
   },
   "source": [
    "**Impact of parallelization on energy consumption**\n",
    "\n",
    "Here we will consider that the total energy consumed by a given computation can be approximated by $E = \\Delta P\\times T$, with $E$ the energy in Joules, $\\Delta P$ the increase in power draw compared to the system baseline in Watts, and $T$ the total time of the computation.\n",
    "\n",
    "While we can't observe it directly from the notebook, the energy consumption of a CPU usually increases with the number of physical cores in use following a logarithmic scale. For the sake of the example, we will consider that $P_T = P_1 \\times T^{0.5}$ regardless of the type of core, with T the number of threads/cores in use. We also consider that $P=\\Delta P$. We now need to identify what is the best setup in terms of energy consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a1a62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "c96a1a62",
    "outputId": "b82448f3-bc41-4dab-d1aa-43c5b2ce0048"
   },
   "outputs": [],
   "source": [
    "nb_threads = int(np.shape(para_sgemm_speedup_curve)[0])\n",
    "\n",
    "cpu_power_curve = [T**0.6 for T in range(1,nb_threads+1)]\n",
    "\n",
    "sgemm_energy_ratio_curve = 1/para_sgemm_speedup_curve[:,1] * cpu_power_curve\n",
    "matmul_v6_energy_ratio_curve = 1/para_matmul_v6_speedup_curve[:,1] * cpu_power_curve\n",
    "\n",
    "plt.plot(para_sgemm_speedup_curve[:,0], sgemm_energy_ratio_curve, c=\"C0\", label=\"OpenBLAS SGEMM para\")\n",
    "plt.plot(para_matmul_v6_speedup_curve[:,0], matmul_v6_energy_ratio_curve, c=\"C1\", label=\"matmul_v6_para\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(\"N of CPU threads\")\n",
    "plt.gca().set_ylabel(\"Energy ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbdd3f",
   "metadata": {
    "id": "7ccbdd3f"
   },
   "source": [
    "This example illustrates that a well-parallelized program running over multiple physical cores is usually more energy efficient than the equivalent single-core program. For real-life applications, the evolution of the energy consumed as a function of the number of threads will be more complicated, with variations due, for example, to different types of cores present in the same CPU and to the possible presence of hyperthreading. The performance curve of a given program can also vary significantly from one system to another and the performance scaling with the number of core can change as well. In practice, it is necessary to measure directly the power draw of the system running the desired program to compute the energy efficiency curve and identify the best set"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
